

## ğŸ“˜ `README.md` â€“ Full Professional Documentation

```markdown
# ğŸ“š RAG-Based Book Summarizer using Groq + LLaMA 4

This project is a **Retrieval-Augmented Generation (RAG)** based pipeline for summarizing large-scale documents (like 600-page books) into ~100-page, structured summaries. It leverages **Groqâ€™s high-speed LLaMA 4 model**, **FAISS vector store**, and **SentenceTransformer embeddings** to enhance summarization accuracy by grounding responses in source material.

---

## ğŸš€ Features

- âœ… Summarizes lengthy books into condensed structured summaries
- ğŸ§  Uses **RAG** to fetch and inject context into the LLM
- âš¡ Powered by **Groq** with `meta-llama/llama-4-scout-17b-16e-instruct`
- ğŸ“„ PDF parsing and chunking support
- ğŸ” FAISS + SentenceTransformer for fast vector search

---

## ğŸ“¦ Tech Stack

- `Groq API` (for summarization)
- `SentenceTransformers` for embeddings
- `FAISS` (vector store)
- `fpdf` (PDF output of summary)
- `PyMuPDF` / `pdfplumber` for PDF parsing
- `dotenv` for secure key loading

---

```
## ğŸ“ Folder Structure
```markdown
BOOK\_SUMMARIZER-RAG/
â”œâ”€â”€ **pycache**/                  # Compiled cache
â”œâ”€â”€ data/                         # Input PDFs
â”œâ”€â”€ output/                       # Final summaries
â”œâ”€â”€ .env                          # API key file (add your GROQ key here)
â”œâ”€â”€ main.py                       # Pipeline controller
â”œâ”€â”€ model.py                      # Summarization logic with Groq
â”œâ”€â”€ pdf_utils.py                  # PDF loader & chunker
â”œâ”€â”€ prompts.py                    # Prompt templates for RAG
â”œâ”€â”€ rag.py                        # RAG flow with FAISS + retriever
â”œâ”€â”€ requirements.txt              # Python dependencies

```

---

## ğŸ” Environment Setup

Create a `.env` file:
```

GROQ\_API\_KEY=your\_groq\_api\_key\_here

````

---

## âš™ï¸ Installation

```bash
git clone https://github.com/your-username/rag-book-summarizer.git
cd rag-book-summarizer
pip install -r requirements.txt
````

---

## ğŸ§  How It Works

1. **PDF is loaded** and split into manageable chunks
2. **Embeddings** are generated using `all-MiniLM-L6-v2`
3. Chunks are stored and indexed in **FAISS vector DB**
4. For each summary prompt:

   * Relevant chunks are retrieved
   * Prompt + context are sent to **Groq** API
5. Summaries are written to a 100-page PDF

---

## ğŸ“Œ Usage

```bash
python main.py
```

Check your output in the `output/` folder.

---

## ğŸ“ˆ Model & API Details

* Model: `meta-llama/llama-4-scout-17b-16e-instruct` via Groq
* Free Tier Limits:

  * ğŸ§  30K Tokens / Min
  * ğŸ” 30 Requests / Min
  * ğŸ“… 500K Tokens / Day

---

## ğŸ“‹ Example Output

* Summary PDF with structured chapters
* 100 pages of condensed content from \~500+ pages source
* Retains original meaning, key events, and flow

---

## ğŸ“œ License

[MIT License](LICENSE)

---



